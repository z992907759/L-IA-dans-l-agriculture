import os
import json
import re
import fitz  # PyMuPDF è§£æ PDF
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# âœ… FAISS å­˜å‚¨è·¯å¾„
FAISS_INDEX_PATH = os.path.join(".", "database", "faiss_db")
DATA_FOLDER = os.path.join(".", "database", "knowledge_base")


def clean_text(text):
    """âœ… æ¸…ç†æ–‡æœ¬"""
    text = re.sub(r"\s{2,}", " ", text)  # å¤šä½™ç©ºæ ¼
    text = re.sub(r"\n{2,}", "\n", text)  # å¤šä½™æ¢è¡Œ
    return text.strip()


def process_pdf(file_path):
    """âœ… è§£æ PDF å¹¶è½¬æ¢ä¸ºé—®ç­”æ ¼å¼"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† PDF: {file_path}")
    doc = fitz.open(file_path)
    documents = []

    for page in doc:
        text = clean_text(page.get_text("text").strip())
        if len(text) > 100:  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
            question = f"What information does this text provide? {text[:100]}..."
            documents.append(Document(page_content=f"Question: {question}\nAnswer: {text}"))

    return documents


def process_txt(file_path):
    """âœ… è§£æ TXT æ–‡ä»¶å¹¶è½¬æ¢ä¸ºé—®ç­”æ ¼å¼"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† TXT: {file_path}")
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        text = clean_text(f.read())

    if len(text) < 100:
        return []  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬

    question = f"What information does this text provide? {text[:100]}..."
    return [Document(page_content=f"Question: {question}\nAnswer: {text}")]


def process_json(file_path):
    """âœ… è§£æ JSON æ–‡ä»¶ï¼Œå¹¶è½¬æ¢ä¸ºé—®ç­”æ ¼å¼"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† JSON: {file_path}")
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents = []
    for entry in data:
        question = entry.get("question", "").strip()
        answer = entry.get("answer", "").strip()

        if question and answer:
            documents.append(Document(page_content=f"Question: {question}\nAnswer: {answer}"))

    return documents


def create_faiss_index():
    """âœ… é‡æ–°å¤„ç†æ‰€æœ‰ `knowledge_base/` æ–‡ä»¶ï¼Œç”Ÿæˆ FAISS ç´¢å¼•"""
    print("ğŸš€ æ­£åœ¨åˆ›å»º FAISS ç´¢å¼•...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
                                       model_kwargs={"device": device})

    all_docs = []
    for file in os.listdir(DATA_FOLDER):
        file_path = os.path.join(DATA_FOLDER, file)
        if file.endswith(".pdf"):
            all_docs.extend(process_pdf(file_path))
        elif file.endswith(".txt"):
            all_docs.extend(process_txt(file_path))
        elif file.endswith(".json"):
            all_docs.extend(process_json(file_path))

    if all_docs:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
        split_docs = text_splitter.split_documents(all_docs)

        vectorstore = FAISS.from_documents(split_docs, embeddings)
        vectorstore.save_local(FAISS_INDEX_PATH)

        print(f"âœ… FAISS ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå­˜å…¥ {len(all_docs)} æ¡æ•°æ®ã€‚")
    else:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å†…å®¹å¯å­˜å…¥ FAISSã€‚")


if __name__ == "__main__":
    create_faiss_index()
