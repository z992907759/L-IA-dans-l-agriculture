import os
import torch
import pandas as pd
import faiss
import numpy as np
from model_evaluator import ModelEvaluator
from langchain_community.embeddings import HuggingFaceEmbeddings
import math
from bert_score import score
from tqdm import tqdm
import time

def main():
    MODEL_PATH = "./model/llama3_lora_colab"
    DATASET_PATH = "./data/agriculture_qa.csv"
    df = pd.read_csv(DATASET_PATH)

    if 'question' not in df.columns or 'answers' not in df.columns:
        raise ValueError(f"âš ï¸ æ•°æ®é›†å¿…é¡»åŒ…å« 'question' å’Œ 'answers' åˆ—ï¼")

    df.rename(columns={'question': 'text'}, inplace=True)
    test_size = min(len(df), 100)
    test_samples = df.sample(n=test_size, random_state=42)

    index_path = "./database/faiss_db/index.faiss"
    if not os.path.exists(index_path):
        raise FileNotFoundError(f"âš ï¸ FAISS ç´¢å¼•æ–‡ä»¶æœªæ‰¾åˆ°ï¼")

    index = faiss.read_index(index_path)
    faiss.omp_set_num_threads(1)

    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    def retrieve_faiss(query_text, top_k=5):
        # print(f"ğŸ” Querying FAISS: {query_text}")
        query_vector = embedder.embed_query(query_text)
        query_vector = np.array(query_vector).astype("float32").reshape(1, -1)
        retrieved_texts = []

        try:
            _, I = index.search(query_vector, k=top_k)
            for idx in I[0]:
                if 0 <= idx < len(df):
                    text = df.iloc[idx]["text"]
                    if query_text.lower() in text.lower():
                        retrieved_texts.append(text.split("\nAnswer: ")[-1].strip())

        except Exception as e:
            print(f"âš ï¸ FAISS æŸ¥è¯¢å¤±è´¥: {e}")
            return "No relevant information found."

        return " ".join(retrieved_texts[:1]) if retrieved_texts else "No relevant information found."

    test_dataset = []
    for _, row in test_samples.iterrows():
        # if len(test_dataset) >= 50: break
        retrieved_text = retrieve_faiss(row["text"])
        if retrieved_text:
            test_dataset.append({"text": retrieved_text, "answers": row["answers"]})

    print(f"âœ… å·²ä» {DATASET_PATH} é€šè¿‡ FAISS å¬å› {len(test_dataset)} æ¡æµ‹è¯•æ•°æ®")

    evaluator = ModelEvaluator(MODEL_PATH, test_dataset=test_dataset)

    def generate_response(prompt):
        """ âœ… ä¿®å¤ `attention_mask` é”™è¯¯ """
        inputs = evaluator.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding="longest"
        ).to(evaluator.device)

        output = evaluator.model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=150,
            pad_token_id=evaluator.tokenizer.eos_token_id
        )

        return evaluator.tokenizer.decode(output[0], skip_special_tokens=True)

    def calculate_perplexity(text):
        """ âœ… è®¡ç®— Perplexityï¼ˆPPLï¼‰"""
        inputs = evaluator.tokenizer(text, return_tensors="pt").to(evaluator.device)

        with torch.no_grad():
            outputs = evaluator.model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss.item()
        return math.exp(loss)  # PPL = e^(loss)

    references = [sample["answers"] for sample in test_dataset]
    predictions = [generate_response(sample["text"]) for sample in tqdm(test_dataset, desc="Generating predictions")]

    import time
    start = time.time()
    print("ğŸ“Š æ­£åœ¨è®¡ç®— BERTScoreï¼Œè¯·ç¨ç­‰ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼‰...")
    # âœ… è®¡ç®— BERTScore
    bert_score = score(predictions, references, lang="en", model_type="bert-base-uncased")[2].mean().item()
    print(f"âœ… BERTScore è®¡ç®—å®Œæˆï¼Œç”¨æ—¶ {time.time() - start:.2f} ç§’")

    # âœ… è®¡ç®— Perplexityï¼ˆPPLï¼‰
    perplexities = [calculate_perplexity(pred) for pred in tqdm(predictions, desc="Calculating Perplexity")]
    avg_ppl = sum(perplexities) / len(perplexities)

    print(f"\nğŸš€ Final evaluation results:")
    print(f"âœ… BERTScore: {bert_score:.4f}")
    print(f"âœ… Perplexity (PPL): {avg_ppl:.4f}")

if __name__ == "__main__":
    main()