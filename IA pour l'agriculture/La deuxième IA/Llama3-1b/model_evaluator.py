import os
import torch
import math
import time

from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from evaluate import load
import evaluate

class ModelEvaluator:
    def __init__(self, model_path, test_dataset=None):
        """
        âœ… åˆå§‹åŒ–è¯„ä¼°å™¨
        :param model_path: å·²å¾®è°ƒçš„æ¨¡å‹è·¯å¾„
        :param test_dataset: ç”¨äºæµ‹è¯•çš„æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
        """
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ… å½“å‰è®¾å¤‡: MPS (Apple GPU)")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ å½“å‰è®¾å¤‡: CPU")

        # âœ… åŠ è½½æ¨¡å‹ï¼ˆç¡®ä¿ float32ï¼Œä¸ä½¿ç”¨ float16ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32
        ).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        # âœ… æ›¿æ¢ `load_metric()` ä¸º `evaluate.load()`
        self.bleu = evaluate.load("bleu")
        self.rouge = evaluate.load("rouge")

        self.test_dataset = test_dataset  # âœ… æµ‹è¯•æ•°æ®é›†

    def calculate_perplexity(self, text):
        """ âœ… è®¡ç®— Perplexityï¼ˆPPLï¼‰ï¼Œç¡®ä¿æ•°æ®åœ¨ CPU """
        encodings = self.tokenizer(text, return_tensors="pt").to(self.device)  # âœ… å¼ºåˆ¶åˆ° CPU

        with torch.no_grad():
            outputs = self.model(**encodings, labels=encodings["input_ids"])
        loss = outputs.loss.item()
        return math.exp(loss)

    def evaluate_generation(self, reference, generated):
        """ âœ… è®¡ç®— BLEU å’Œ ROUGE åˆ†æ•°ï¼Œä¿®æ­£è¾“å…¥æ ¼å¼ """
        bleu_score = self.bleu.compute(
            predictions=[generated],  # âœ… ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²åˆ—è¡¨
            references=[[reference]]  # âœ… å‚è€ƒç­”æ¡ˆæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨
        )
        rouge_score = self.rouge.compute(
            predictions=[generated],
            references=[reference]
        )

        return {
            "BLEU": bleu_score["bleu"],
            "ROUGE": rouge_score
        }

    def test_on_dataset(self):
        """ âœ… åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ """
        if not self.test_dataset:
            print("âš ï¸ æ²¡æœ‰æä¾›æµ‹è¯•é›†ï¼Œè·³è¿‡æµ‹è¯•é›†è¯„ä¼°")
            return None

        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for sample in tqdm(self.test_dataset, desc="Evaluating Test Set"):
                text = sample["text"]
                encodings = self.tokenizer(text, return_tensors="pt").to(self.device)  # âœ… ç¡®ä¿æ•°æ®åœ¨ CPU
                outputs = self.model(**encodings, labels=encodings["input_ids"])
                loss = outputs.loss.item()

                total_loss += loss
                num_batches += 1

        avg_loss = total_loss / num_batches
        ppl = math.exp(avg_loss)  # è®¡ç®—å¹³å‡ PPL

        return {
            "Test Loss": avg_loss,
            "Test Perplexity (PPL)": ppl
        }

    def run_evaluation(self, sample_text, reference_text=None, generated_text=None):
        """
        âœ… è¿è¡Œæ‰€æœ‰è¯„ä¼°æ–¹æ³•
        :param sample_text: ç”¨äºè®¡ç®— PPL çš„ç¤ºä¾‹æ–‡æœ¬
        :param reference_text: å‚è€ƒç­”æ¡ˆï¼ˆå¦‚æœè¯„ä¼°æ–‡æœ¬ç”Ÿæˆï¼‰
        :param generated_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        :return: è¯„ä¼°ç»“æœå­—å…¸
        """
        results = {}

        # 1ï¸âƒ£ è®¡ç®— Perplexityï¼ˆPPLï¼‰
        ppl = self.calculate_perplexity(sample_text)
        results["Perplexity"] = ppl
        print(f"âœ… Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰: {ppl:.4f}")

        # 2ï¸âƒ£ è®¡ç®— BLEU / ROUGEï¼ˆå¦‚æœæä¾›äº†ç”Ÿæˆæ–‡æœ¬ï¼‰
        if reference_text and generated_text:
            gen_scores = self.evaluate_generation(reference_text, generated_text)
            results["BLEU"] = gen_scores["BLEU"]
            results["ROUGE"] = gen_scores["ROUGE"]
            print(f"âœ… BLEU Score: {gen_scores['BLEU']:.4f}")
            print(f"âœ… ROUGE Score: {gen_scores['ROUGE']}")

            # âœ… è®¡ç®— BERTScore
            start = time.time()
            print("ğŸ“Š æ­£åœ¨è®¡ç®— BERTScoreï¼Œè¯·ç¨ç­‰ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼‰...")
            references = [sample["answers"] for sample in self.test_dataset]
            predictions = [generate_response(sample["text"]) for sample in tqdm(self.test_dataset, desc="Generating predictions")]
            bert_score = score(predictions, references, lang="en", model_type="bert-base-uncased")[2].mean().item()
            print(f"âœ… BERTScore è®¡ç®—å®Œæˆï¼Œç”¨æ—¶ {time.time() - start:.2f} ç§’")

        # 3ï¸âƒ£ è®¡ç®—æµ‹è¯•é›†ä¸Šçš„æŸå¤±å’Œ PPL
        test_results = self.test_on_dataset()
        if test_results:
            results.update(test_results)
            print(f"âœ… æµ‹è¯•é›†å¹³å‡æŸå¤±: {test_results['Test Loss']:.4f}")
            print(f"âœ… æµ‹è¯•é›† Perplexityï¼ˆPPLï¼‰: {test_results['Test Perplexity (PPL)']:.4f}")

        return results