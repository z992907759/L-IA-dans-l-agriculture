import os
import torch
from gtts import gTTS
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime


class LoRAModel:
    def __init__(self, base_model_name="meta-llama/Llama-3.2-1B-Instruct",
                 lora_model_path="./Model/llama3_lora_colab"):
        """
        âœ… LoRA å¾®è°ƒæ¨¡å‹çš„å°è£…ç±»
        âœ… æ”¯æŒåŠ è½½ LLaMA-3 + LoRA é€‚é…å±‚
        âœ… æ”¯æŒ GPU åŠ é€Ÿ
        âœ… æ”¯æŒç”¨æˆ·äº¤äº’å¼å¯¹è¯
        âœ… ç”Ÿæˆè¯­éŸ³å›å¤ (gTTS)
        âœ… è‡ªåŠ¨æ¸…ç©º audio æ–‡ä»¶å¤¹
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path

        # âœ… ç¡®ä¿éŸ³é¢‘å­˜å‚¨æ–‡ä»¶å¤¹å­˜åœ¨
        self.audio_folder = "audio"
        os.makedirs(self.audio_folder, exist_ok=True)

        # âœ… æ¸…ç©º audio æ–‡ä»¶å¤¹
        self.clear_audio_folder()

        # âœ… åŠ è½½æ¨¡å‹
        self.load_model()

    def clear_audio_folder(self):
        """âœ… æ¸…ç©º audio æ–‡ä»¶å¤¹"""
        print("ğŸ—‘ï¸ æ­£åœ¨æ¸…ç©º audio æ–‡ä»¶å¤¹...")
        for file in os.listdir(self.audio_folder):
            file_path = os.path.join(self.audio_folder, file)
            if file.endswith(".mp3"):  # ä»…åˆ é™¤ MP3 æ–‡ä»¶
                os.remove(file_path)
        print("âœ… audio æ–‡ä»¶å¤¹å·²æ¸…ç©ºï¼")

    def load_model(self):
        """âœ… åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹ï¼Œå¹¶åˆå¹¶ LoRA é€‚é…å±‚"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        ).to(self.device)

        print("ğŸ”„ Loading LoRA adaptation layer...")
        self.lora_model = PeftModel.from_pretrained(self.base_model, self.lora_model_path)

        print("ğŸ”„ Merging LoRA adaptation layer...")
        self.model = self.lora_model.merge_and_unload().to(self.device)

        # âœ… åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        print("âœ… Model loading completed!")

    def generate_response(self, prompt):
        """âœ… ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶è°ƒç”¨ gTTS ç”ŸæˆéŸ³é¢‘"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 1024,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # âœ… ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        audio_path = self.generate_audio(response_text)

        return response_text, audio_path

    def generate_audio(self, text):
        """âœ… ä½¿ç”¨ gTTS ç”ŸæˆéŸ³é¢‘"""
        print("ğŸ”Š æ­£åœ¨ç”Ÿæˆè¯­éŸ³...")

        # âœ… ç”Ÿæˆå”¯ä¸€çš„éŸ³é¢‘æ–‡ä»¶å
        filename = f"response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp3"
        audio_path = os.path.join(self.audio_folder, filename)

        # âœ… ç”ŸæˆéŸ³é¢‘
        tts = gTTS(text=text, lang="en")  # è¯­è¨€å¯æ”¹ä¸º "zh-CN" (ä¸­æ–‡) æˆ–å…¶ä»–æ”¯æŒçš„è¯­è¨€
        tts.save(audio_path)

        print(f"âœ… è¯­éŸ³å·²ç”Ÿæˆ: {audio_path}")
        return audio_path

    def chat(self):
        """âœ… äº¤äº’å¼èŠå¤©ï¼Œæ”¯æŒæ–‡æœ¬+è¯­éŸ³"""
        print("\nğŸ¤– LLaMA-3 LoRA model launched! Type 'exit' to exit.\n")
        while True:
            user_input = input("ğŸŸ¢ You: ")
            if user_input.lower() == "exit":
                print("ğŸ‘‹ Good Byeï¼")
                break
            response, audio_file = self.generate_response(user_input)
            print(f"ğŸ¤– AI: {response}\n")

            # âœ… é€‰æ‹©æ˜¯å¦æ’­æ”¾éŸ³é¢‘
            # play_audio = input("ğŸµ æ’­æ”¾è¯­éŸ³ï¼Ÿ(y/n): ").strip().lower()
            # if play_audio == "y":
            #     os.system(f"open {audio_file}")  # Mac
                # os.system(f"start {audio_file}")  # Windows
                # os.system(f"xdg-open {audio_file}")  # Linux


# âœ… è¿è¡Œäº¤äº’å¼èŠå¤©
if __name__ == "__main__":
    llama = LoRAModel()
    llama.chat()