import re  # âœ… éœ€è¦æ‰‹åŠ¨å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
import os
import torch
import glob
from gtts import gTTS
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


class LoRAModel:
    def __init__(self, base_model_name="meta-llama/Llama-3.2-1B-Instruct",
                 lora_model_path="./model/llama3_lora_colab",
                 faiss_index_path="./database/faiss_db"):
        """
        âœ… LoRA å¾®è°ƒæ¨¡å‹çš„å°è£…ç±»ï¼Œæ”¯æŒ FAISS å…ˆæŸ¥è¯¢æ•°æ®åº“
        âœ… ç»“åˆ FAISS æ•°æ®åº“ + LLaMA-3 ç”Ÿæˆç­”æ¡ˆ
        âœ… è¯­éŸ³åˆæˆ (gTTS)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path
        self.faiss_index_path = faiss_index_path

        # âœ… ç¡®ä¿éŸ³é¢‘å­˜å‚¨æ–‡ä»¶å¤¹å­˜åœ¨
        self.audio_folder = "audio"
        os.makedirs(self.audio_folder, exist_ok=True)
        self.clear_audio_folder()

        # âœ… åŠ è½½æ¨¡å‹å’Œ FAISS
        self.load_model()
        self.load_faiss_index()

    def clear_audio_folder(self):
        """âœ… æ¸…ç©º audio æ–‡ä»¶å¤¹"""
        for file in os.listdir(self.audio_folder):
            file_path = os.path.join(self.audio_folder, file)
            if file.endswith(".mp3"):
                os.remove(file_path)

    def load_model(self):
        """âœ… åŠ è½½ LLaMA-3 + LoRA"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        ).to(self.device)

        print("ğŸ”„ åŠ è½½ LoRA é€‚é…å±‚...")
        self.lora_model = PeftModel.from_pretrained(self.base_model, self.lora_model_path)
        self.model = self.lora_model.merge_and_unload().to(self.device)

        # âœ… åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        print("âœ… LLaMA-3 + LoRA åŠ è½½å®Œæˆï¼")

    def load_faiss_index(self):
        """âœ… åŠ è½½ FAISS å‘é‡æ•°æ®åº“"""
        print("ğŸ” æ­£åœ¨åŠ è½½ FAISS ç´¢å¼•...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": self.device}
        )

        self.faiss_store = FAISS.load_local(
            self.faiss_index_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

        print("âœ… FAISS ç´¢å¼•åŠ è½½å®Œæˆï¼")

    def search_faiss(self, query, k=3):
        """âœ… åœ¨ FAISS æ•°æ®åº“ä¸­æŸ¥æ‰¾æœ€ç›¸å…³çš„ k ä¸ªæ–‡æ¡£"""
        print(f"ğŸ” æœç´¢ FAISS: {query}")
        results = self.faiss_store.similarity_search(query, k=k)

        if results:
            return [doc.page_content for doc in results]
        return []

    import os
    from gtts import gTTS  # âœ… ç¡®ä¿å¯¼å…¥ gTTS

    import os
    import re
    import time
    from datetime import datetime
    from gtts import gTTS  # âœ… ç¡®ä¿å¯¼å…¥ gTTS

    def generate_response(self, prompt):
        """âœ… ç»“åˆ FAISS ç»“æœ + LLaMA-3 ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œå¹¶ç”¨ gTTS ç”ŸæˆéŸ³é¢‘"""

        # 1ï¸âƒ£ **æŸ¥è¯¢ FAISS**
        faiss_results = self.search_faiss(prompt, k=3)

        # 2ï¸âƒ£ **æ¸…ç† FAISS æ•°æ®**
        if faiss_results:
            faiss_cleaned = []
            for doc in faiss_results:
                cleaned_text = re.sub(r"\(.*?et al\..*?\)", "", doc)  # âœ… åˆ é™¤ "Lu et al. 2001" è¿™ç§å¼•ç”¨
                cleaned_text = re.sub(r"(\d+\.\s[A-Za-z ]+)", "", cleaned_text)  # âœ… åˆ é™¤ç« èŠ‚æ ‡é¢˜
                cleaned_text = cleaned_text.strip()
                if cleaned_text:
                    faiss_cleaned.append(cleaned_text)

            faiss_summary = " ".join(faiss_cleaned)[:500]  # âœ… é™åˆ¶ FAISS ç‰‡æ®µé•¿åº¦
        else:
            faiss_summary = "No relevant knowledge found in FAISS."

        # 3ï¸âƒ£ **ä¼˜åŒ– Prompt**
        full_prompt = f"""You are an expert in agriculture. Provide a **complete, structured answer** to the user's question.
    Use FAISS knowledge and your expertise to answer in a **detailed yet concise way**.
    Do NOT repeat FAISS text verbatim. Instead, **extract useful insights and explain them naturally**.
    Avoid unnecessary information and ensure the answer flows well.

    ### FAISS Knowledge:
    {faiss_summary}

    ### User Question:
    {prompt}

    ### Answer (Provide all useful details in a well-structured manner, at least 200 words):"""

        print(f"ğŸ“š ç”Ÿæˆçš„ Prompt é•¿åº¦: {len(full_prompt)}")
        print(f"ğŸ“š ç”Ÿæˆçš„ Prompt:\n{full_prompt[:500]}...")  # âœ… é™åˆ¶è¾“å‡ºï¼Œé¿å…è¶…é•¿æ‰“å°

        # 4ï¸âƒ£ **è®© LLaMA-3 ç”Ÿæˆç­”æ¡ˆ**
        inputs = self.tokenizer(full_prompt, return_tensors="pt", truncation=True).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=1024,
                min_length=200,  # âœ… ç¡®ä¿è‡³å°‘ç”Ÿæˆ 200 ä¸ª Tokenï¼Œé¿å…æå‰ç»“æŸ
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # 5ï¸âƒ£ **åˆ é™¤æ¨¡å‹å¯èƒ½å¤è¿°çš„ Prompt**
        response_text = response_text.replace(full_prompt, "").strip()
        response_text = response_text.split("### Answer:")[-1].strip()

        # 6ï¸âƒ£ **ç¡®ä¿ `./audio/` ç›®å½•å­˜åœ¨**
        os.makedirs("./audio", exist_ok=True)

        # 7ï¸âƒ£ **ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºæ–‡ä»¶å**
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  # ç”Ÿæˆ `YYYYMMDD_HHMMSS`
        audio_file = f"./audio/{timestamp}.mp3"  # âœ… éŸ³é¢‘æ–‡ä»¶åæ ¼å¼ç¤ºä¾‹ï¼š`./audio/20240318_153045.mp3`

        # 8ï¸âƒ£ **ç”Ÿæˆ gTTS è¯­éŸ³å¹¶ä¿å­˜**
        tts = gTTS(response_text, lang="en")
        tts.save(audio_file)
        print(f"ğŸ”Š è¯­éŸ³å·²ç”Ÿæˆå¹¶ä¿å­˜åœ¨: {audio_file}")

        return response_text, audio_file  # âœ… è¿”å›å›ç­”å’ŒéŸ³é¢‘è·¯å¾„

    def chat(self):
        """âœ… äº¤äº’æ¨¡å¼å¯åŠ¨æ—¶ï¼Œæ¸…ç©º ./audio/ ç›®å½•ï¼Œå¹¶å¯åŠ¨å¯¹è¯"""

        # 1ï¸âƒ£ **ç¡®ä¿ `./audio/` ç›®å½•å­˜åœ¨**
        os.makedirs("./audio", exist_ok=True)

        # 2ï¸âƒ£ **æ¸…ç©º `./audio/` ç›®å½•**
        audio_files = glob.glob("./audio/*.mp3")  # æ‰¾åˆ°æ‰€æœ‰ `.mp3` æ–‡ä»¶
        for file in audio_files:
            try:
                os.remove(file)  # âœ… åˆ é™¤éŸ³é¢‘æ–‡ä»¶
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ é™¤ {file}: {e}")

        print("ğŸ—‘ï¸ å·²æ¸…ç©º `./audio/` æ–‡ä»¶å¤¹ï¼")

        # 3ï¸âƒ£ **è¿›å…¥å¯¹è¯æ¨¡å¼**
        print("\nğŸ¤– äº¤äº’æ¨¡å¼å¯åŠ¨ï¼è¾“å…¥ 'exit' é€€å‡ºã€‚\n")

        while True:
            user_input = input("ğŸŸ¢ You: ").strip()
            if user_input.lower() == "exit":
                print("ğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼ï¼")
                break

            response, audio_file = self.generate_response(user_input)
            print(f"\nğŸ¤– AI: {response}\n")


if __name__ == "__main__":
    llama = LoRAModel()
    llama.chat()
