import torch
import os
import shutil
import pyttsx3
import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from gtts import gTTS
import torch


class LoRAModel:
    def __init__(self, base_model_name="meta-llama/Llama-3.2-1B-Instruct",
                 lora_model_path="./Model/llama3_lora_colab",
                 audio_dir="./audio"):
        """
        âœ… LoRA å¾®è°ƒæ¨¡å‹çš„å°è£…ç±»
        âœ… æ”¯æŒåŠ è½½ LLaMA-3 + LoRA é€‚é…å±‚
        âœ… æ”¯æŒ GPU åŠ é€Ÿ
        âœ… æ”¯æŒç”¨æˆ·äº¤äº’å¼å¯¹è¯
        âœ… æ”¯æŒç”Ÿæˆè¯­éŸ³éŸ³é¢‘
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path
        self.audio_dir = audio_dir

        # âœ… åˆå§‹åŒ–TTSå¼•æ“
        self.tts_engine = pyttsx3.init()
        self.tts_engine.setProperty('rate', 150)  # è¯­é€Ÿ
        self.tts_engine.setProperty('volume', 1.0)  # éŸ³é‡

        # âœ… æ¸…ç©ºéŸ³é¢‘ç›®å½•
        self.clear_audio_directory()

        # âœ… åŠ è½½æ¨¡å‹
        self.load_model()

        # âœ… æ‰“å°å½“å‰è®¾å¤‡è¿è¡Œç±»å‹
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        if self.device == "cuda":
            print(f"ğŸ” å½“å‰è®¾å¤‡: GPU ({torch.cuda.get_device_name(0)})")
            print(f"ğŸ› ï¸ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"ğŸ’¾ å¯ç”¨æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
        else:
            print("ğŸ” å½“å‰è®¾å¤‡: CPU")

    def clear_audio_directory(self):
        """âœ… æ¸…ç©º `audio/` ç›®å½•"""
        if os.path.exists(self.audio_dir):
            shutil.rmtree(self.audio_dir)
        os.makedirs(self.audio_dir, exist_ok=True)

    def load_model(self):
        """âœ… åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹ï¼Œå¹¶åˆå¹¶ LoRA é€‚é…å±‚"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        ).to(self.device)

        print("ğŸ”„ Loading LoRA adaptation layer...")
        self.lora_model = PeftModel.from_pretrained(self.base_model, self.lora_model_path)

        print("ğŸ”„ Merging LoRA adaptation layer...")
        self.model = self.lora_model.merge_and_unload().to(self.device)

        # âœ… åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        print("âœ… Model loading completed!")

    def generate_response(self, prompt):
        """âœ… ä¼˜åŒ– Promptï¼Œå‡å°‘ AI å¤è¿°é—®é¢˜çš„æ¦‚ç‡"""
        optimized_prompt = f"Answer directly without repeating the question:\n{prompt}"  # å…³é”®ç‚¹
        inputs = self.tokenizer(optimized_prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 1024,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,  # âœ… å¢å¼ºæƒ©ç½šï¼Œå‡å°‘é‡å¤
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # âœ… è¿›ä¸€æ­¥å»é™¤ AI å¤è¿°çš„éƒ¨åˆ†
        if response_text.lower().startswith(prompt.lower()):  # å¦‚æœå›ç­”å‰åŠéƒ¨åˆ†æ˜¯é—®é¢˜
            response_text = response_text[len(prompt):].strip()  # å»æ‰é—®é¢˜éƒ¨åˆ†

        print(f"ğŸ¤– AI: {response_text}")
        self.generate_audio(response_text)
        return response_text

    def generate_audio(self, text):
        """âœ… ä½¿ç”¨ gTTS ç”Ÿæˆè¯­éŸ³ï¼Œå¹¶ä¿å­˜ä¸º MP3 æ–‡ä»¶"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        audio_path = os.path.join(self.audio_dir, f"audio_{timestamp}.mp3")

        try:
            # ç”ŸæˆéŸ³é¢‘ï¼ˆè¯­è¨€é»˜è®¤æ˜¯è‹±è¯­ï¼Œä½ å¯ä»¥æ”¹æˆ 'zh-cn' ï¼‰
            tts = gTTS(text=text, lang="en")  # å¦‚æœè¦ç”¨ä¸­æ–‡ï¼Œæ”¹æˆ lang="zh-cn"
            tts.save(audio_path)
            print(f"ğŸ”Š è¯­éŸ³å·²ç”Ÿæˆ: {audio_path}")

        except Exception as e:
            print(f"âŒ è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")

    def chat(self):
        """âœ… æ”¯æŒç”¨æˆ·äº¤äº’å¼èŠå¤©"""
        print("\nğŸ¤– LLaMA-3 LoRA model launched! Type 'exit' to exit.\n")
        while True:
            user_input = input("ğŸŸ¢ You: ")
            if user_input.lower() == "exit":
                print("ğŸ‘‹ Good Byeï¼")
                break
            response = self.generate_response(user_input)


# âœ… è¿è¡Œäº¤äº’å¼èŠå¤©
if __name__ == "__main__":
    llama = LoRAModel()
    llama.chat()
