import re  # âœ… éœ€è¦æ‰‹åŠ¨å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
import os
import torch
import glob
from gtts import gTTS
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datetime import datetime
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import time  # âœ… éœ€è¦å¯¼å…¥ `time` ä»¥è®¡ç®—ç”Ÿæˆæ—¶é—´


class LoRAModel:
    def __init__(self, base_model_name="meta-llama/Llama-3.2-1B-Instruct",
                 lora_model_path="./model/llama3_lora_colab",
                 faiss_index_path="./database/faiss_db"):
        """
        âœ… Encapsulation class for LoRA fine-tuning, supporting FAISS database queries| LoRA å¾®è°ƒæ¨¡å‹çš„å°è£…ç±»ï¼Œæ”¯æŒ FAISS å…ˆæŸ¥è¯¢æ•°æ®åº“
        âœ… Generating answers by combining FAISS database with LLaMA-3 | ç»“åˆ FAISS æ•°æ®åº“ + LLaMA-3 ç”Ÿæˆç­”æ¡ˆ
        âœ… Text-to-speech synthesis (gTTS) |è¯­éŸ³åˆæˆ (gTTS)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path
        self.faiss_index_path = faiss_index_path

        # âœ…  Ensure that the audio storage folder exists. ç¡®ä¿éŸ³é¢‘å­˜å‚¨æ–‡ä»¶å¤¹å­˜åœ¨
        self.audio_folder = "audio"
        os.makedirs(self.audio_folder, exist_ok=True)
        self.clear_audio_folder()

        # âœ…  Load the model and FAISS.| åŠ è½½æ¨¡å‹å’Œ FAISS
        self.load_model()
        self.load_faiss_index()

    def clear_audio_folder(self):
        """âœ… å¯åŠ¨æ—¶æ¸…ç©º `./audio/` ç›®å½•"""
        audio_files = glob.glob(os.path.join(self.audio_folder, "*.mp3"))
        for file in audio_files:
            try:
                os.remove(file)
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ é™¤ {file}: {e}")  # åˆ é™¤å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯
        print("ğŸ—‘ï¸The `./audio/` directory has been cleared! | `./audio/` ç›®å½•å·²æ¸…ç©ºï¼")  # âœ… æç¤ºæ¸…ç†å®Œæˆ

    def load_model(self):
        """âœ…â€œLoad LLaMA-3 + LoRA.| åŠ è½½ LLaMA-3 + LoRA"""
        print("ğŸ”„Loading the LLaMA-3 base modelâ€¦  | æ­£åœ¨åŠ è½½ LLaMA-3 åŸºç¡€æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        ).to(self.device)

        print("ğŸ”„ Loading the LoRA adapter layersâ€¦ | åŠ è½½ LoRA é€‚é…å±‚...")
        self.lora_model = PeftModel.from_pretrained(self.base_model, self.lora_model_path)
        self.model = self.lora_model.merge_and_unload().to(self.device)

        # âœ… åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        print("âœ…LLaMA-3 + LoRA loaded successfully! | LLaMA-3 + LoRA åŠ è½½å®Œæˆï¼")

    def load_faiss_index(self):
        """âœ…Load the FAISS vector database.| åŠ è½½ FAISS å‘é‡æ•°æ®åº“"""
        print("ğŸ” Loading the FAISS indexâ€¦ | æ­£åœ¨åŠ è½½ FAISS ç´¢å¼•...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": self.device}
        )

        self.faiss_store = FAISS.load_local(
            self.faiss_index_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

        print("âœ…FAISS index loaded successfully! | FAISS ç´¢å¼•åŠ è½½å®Œæˆï¼")

    def search_faiss(self, query, k=3):
        """âœ…Retrieve the top k most relevant documents from the FAISS database. | åœ¨ FAISS æ•°æ®åº“ä¸­æŸ¥æ‰¾æœ€ç›¸å…³çš„ k ä¸ªæ–‡æ¡£"""
        print(f"ğŸ” Serach FAISS: {query}")
        results = self.faiss_store.similarity_search(query, k=k)

        if results:
            return [doc.page_content for doc in results]
        return []

    def generate_response(self, prompt):
        """âœ… ç»“åˆ FAISS + LLaMA-3 ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œå¹¶è®¡ç®— Token é€Ÿç‡"""

        # 1ï¸âƒ£ **æŸ¥è¯¢ FAISS**
        faiss_results = self.search_faiss(prompt, k=3)

        # 2ï¸âƒ£ **æ¸…ç† FAISS ç»“æœ**
        if faiss_results:
            faiss_cleaned = []
            for doc in faiss_results:
                cleaned_text = re.sub(r"\(.*?et al\..*?\)", "", doc)  # âœ… åˆ é™¤å¼•ç”¨
                cleaned_text = re.sub(r"(\d+\.\s[A-Za-z ]+)", "", cleaned_text)  # âœ… åˆ é™¤ç« èŠ‚æ ‡é¢˜
                cleaned_text = cleaned_text.strip()
                if cleaned_text:
                    faiss_cleaned.append(cleaned_text)

            faiss_summary = " ".join(faiss_cleaned)[:500]  # âœ… é™åˆ¶ FAISS ç‰‡æ®µé•¿åº¦
        else:
            faiss_summary = "No relevant knowledge found in FAISS."

        # 3ï¸âƒ£ **ä¼˜åŒ– Prompt**
        full_prompt = f"""You are an expert in agriculture. Provide a **complete, structured answer** to the user's question.
        Use FAISS knowledge and your expertise to answer in a **detailed yet concise way**.
        Do NOT repeat FAISS text verbatim. Instead, **extract useful insights and explain them naturally**.
        Avoid unnecessary information and ensure the answer flows well.

        ### FAISS Knowledge:
        {faiss_summary}

        ### User Question:
        {prompt}

        ### Answer (Provide all useful details in a well-structured manner, at least 200 words):"""

        print(f"ğŸ“š Length of the generated prompt: {len(full_prompt)}")
        print(f"ğŸ“š Generated prompt preview:\n{full_prompt[:500]}...")

        # 4ï¸âƒ£ **è®© LLaMA-3 ç”Ÿæˆç­”æ¡ˆ**
        inputs = self.tokenizer(full_prompt, return_tensors="pt", truncation=True).to(self.device)

        # âœ… **è®°å½•å¼€å§‹æ—¶é—´**
        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=1024,
                min_length=200,  # âœ… ç¡®ä¿è‡³å°‘ç”Ÿæˆ 200 ä¸ª Tokenï¼Œé¿å…æå‰ç»“æŸ
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # âœ… **è®°å½•ç»“æŸæ—¶é—´**
        end_time = time.time()

        # âœ… **è®¡ç®—ç”Ÿæˆæ—¶é—´**
        generation_time = end_time - start_time  # è®¡ç®—æ€»æ—¶é—´ï¼ˆç§’ï¼‰

        # âœ… **è®¡ç®—ç”Ÿæˆ Token æ•°**
        num_tokens = outputs.shape[1]  # è·å–ç”Ÿæˆçš„ Token æ•°é‡

        # âœ… **è®¡ç®—æ¯ç§’ Token é€Ÿç‡**
        tokens_per_second = num_tokens / generation_time if generation_time > 0 else 0

        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # 5ï¸âƒ£ **åˆ é™¤æ¨¡å‹å¯èƒ½å¤è¿°çš„ Prompt**
        response_text = response_text.replace(full_prompt, "").strip()
        response_text = response_text.split("### Answer:")[-1].strip()

        # âœ… **æ‰“å°ç”Ÿæˆ Token é€Ÿç‡**
        print(f"ğŸš€ Generation completed!")
        print(f"âœ… Number of tokens generated: {num_tokens}")
        print(f"âœ… Generated at: {generation_time:.2f} ç§’")
        print(f"âœ… Tokens generated per second: {tokens_per_second:.2f} tokens/sec")

        # 6ï¸âƒ£ **ç¡®ä¿ `./audio/` ç›®å½•å­˜åœ¨**
        os.makedirs("./audio", exist_ok=True)

        # 7ï¸âƒ£ **ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å**
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        audio_file = f"./audio/{timestamp}.mp3"

        # 8ï¸âƒ£ **ç”Ÿæˆ gTTS è¯­éŸ³**
        tts = gTTS(response_text, lang="en")
        tts.save(audio_file)
        print(f"ğŸ”Š The speech has been generated and saved in: {audio_file}")

        return response_text, audio_file  # âœ… è¿”å›å›ç­”å’ŒéŸ³é¢‘è·¯å¾„

    def chat(self):

        # 1ï¸âƒ£ **ç¡®ä¿ `./audio/` ç›®å½•å­˜åœ¨**
        os.makedirs("./audio", exist_ok=True)

        # 3ï¸âƒ£ **è¿›å…¥å¯¹è¯æ¨¡å¼**
        print("\nğŸ¤– Interactive mode started! Type â€˜exitâ€™ or â€˜quit' to quit.\n") # âœ… æç¤ºè¿›å…¥å¯¹è¯æ¨¡å¼

        while True:
            user_input = input("ğŸŸ¢ You: ").strip()
            if user_input.lower() == "exit" or user_input.lower() == "quit":
                print("ğŸ‘‹ Good Byeï¼")
                break

            response, audio_file = self.generate_response(user_input)
            print(f"\nğŸ¤– AI: {response}\n")


if __name__ == "__main__":
    llama = LoRAModel()
    llama.chat()
